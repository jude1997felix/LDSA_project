{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import time\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! rm ./metastore_db/*.lck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.104:7077\") \\\n",
    "        .appName(\"weak-test-one-executor\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 2)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Python API for sparkmeasure package\n",
    "# and attach the sparkMeasure Listener for stagemetrics to the active Spark session\n",
    "\n",
    "from sparkmeasure import StageMetrics\n",
    "stagemetrics = StageMetrics(spark_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cell and line magic to wrap the instrumentation\n",
    "from IPython.core.magic import (register_line_magic, register_cell_magic, register_line_cell_magic)\n",
    "\n",
    "@register_line_cell_magic\n",
    "def sparkmeasure(line, cell=None):\n",
    "    \"run and measure spark workload. Use: %sparkmeasure or %%sparkmeasure\"\n",
    "    val = cell if cell is not None else line\n",
    "    stagemetrics.begin()\n",
    "    eval(val)\n",
    "    stagemetrics.end()\n",
    "    stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagemetrics.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#df = spark.read.json(\"hdfs://192.168.2.104:9000/reddit_sample/*\").cache()\n",
    "df = spark_session.read.json(\"hdfs://192.168.2.104:9000/reddit_sample/pRC_2009*\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_mapper(comment):\n",
    "    # Note: the VADER sentiment analyzer is trained on a single sentence as input\n",
    "    # We take a naive initial approach here and simply treat a comment as single sentence\n",
    "    sentiment = sid.polarity_scores(comment['body'])['compound']\n",
    "    return (comment['score'], sentiment)\n",
    "\n",
    "#nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment = df.rdd \\\n",
    "    .filter(lambda x: len(x['body'])<1000)\\\n",
    "    .map(sentiment_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentiment = sentiment\\\n",
    "    .mapValues(lambda v: (v, 1))\\\n",
    "    .reduceByKey(lambda a,b: (a[0]+b[0], a[1]+b[1]))\\\n",
    "    .mapValues(lambda v: (v[0]/v[1], v[1]))\\\n",
    "    .sortBy(lambda k_v: k_v[1][0], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "876"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_sentiment.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = avg_sentiment.map(lambda x: (x[0], x[1][0])).toDF([\"Score\", \"Sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0530682939971702"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation = df_sentiment.stat.corr(\"Score\", \"Sentiment\")\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 820.7390441894531\n"
     ]
    }
   ],
   "source": [
    "print(\"Time\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 2\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 9\n",
      "sum(numTasks) => 123\n",
      "elapsedTime => 820484 (14 min)\n",
      "sum(stageDuration) => 819387 (14 min)\n",
      "sum(executorRunTime) => 1572399 (26 min)\n",
      "sum(executorCpuTime) => 134650 (2.2 min)\n",
      "sum(executorDeserializeTime) => 3192 (3 s)\n",
      "sum(executorDeserializeCpuTime) => 987 (1.0 s)\n",
      "sum(resultSerializationTime) => 26 (26 ms)\n",
      "sum(jvmGCTime) => 25681 (26 s)\n",
      "sum(shuffleFetchWaitTime) => 2 (2 ms)\n",
      "sum(shuffleWriteTime) => 260 (0.3 s)\n",
      "max(resultSize) => 44051 (43.0 KB)\n",
      "sum(numUpdatedBlockStatuses) => 0\n",
      "sum(diskBytesSpilled) => 0 (0 Bytes)\n",
      "sum(memoryBytesSpilled) => 0 (0 Bytes)\n",
      "max(peakExecutionMemory) => 16320\n",
      "sum(recordsRead) => 7565928\n",
      "sum(bytesRead) => 4463291454 (4.0 GB)\n",
      "sum(recordsWritten) => 0\n",
      "sum(bytesWritten) => 0 (0 Bytes)\n",
      "sum(shuffleTotalBytesRead) => 487169 (475.0 KB)\n",
      "sum(shuffleTotalBlocksFetched) => 1447\n",
      "sum(shuffleLocalBlocksFetched) => 1447\n",
      "sum(shuffleRemoteBlocksFetched) => 0\n",
      "sum(shuffleBytesWritten) => 177687 (173.0 KB)\n",
      "sum(shuffleRecordsWritten) => 880\n"
     ]
    }
   ],
   "source": [
    "stagemetrics.end()\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregated Spark accumulables of type internal.metric. Sum of values grouped by metric name\n",
      "Name => sum(value) [group by name]\n",
      "\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "executorCpuTime => 134653 (2.2 min)\n",
      "executorDeserializeCpuTime => 991 (1.0 s)\n",
      "executorDeserializeTime => 3192 (3 s)\n",
      "executorRunTime => 1572399 (26 min)\n",
      "input.bytesRead => 4463291454 (4.0 GB)\n",
      "input.recordsRead => 7565928\n",
      "jvmGCTime => 25681 (26 s)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 19296\n",
      "resultSerializationTime => 26 (26 ms)\n",
      "resultSize => 252514 (246.0 KB)\n",
      "shuffle.read.fetchWaitTime => 2 (2 ms)\n",
      "shuffle.read.localBlocksFetched => 1447\n",
      "shuffle.read.localBytesRead => 487169 (475.0 KB)\n",
      "shuffle.read.recordsRead => 2347\n",
      "shuffle.read.remoteBlocksFetched => 0\n",
      "shuffle.read.remoteBytesRead => 0 (0 Bytes)\n",
      "shuffle.read.remoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffle.write.bytesWritten => 177687 (173.0 KB)\n",
      "shuffle.write.recordsWritten => 880\n",
      "shuffle.write.writeTime => 262 (0.3 s)\n",
      "\n",
      "SQL Metrics and other non-internal metrics. Values grouped per accumulatorId and metric name.\n",
      "Accid, Name => max(value) [group by accId, name]\n",
      "\n",
      "  449, number of output rows => 3782964\n",
      "  453, duration total => 58350 (58 s)\n",
      "  480, number of output rows => 3782964\n",
      "  484, duration total => 218027 (3.6 min)\n",
      "  485, number of output rows => 3782964\n",
      "  637, duration total => 684 (0.7 s)\n",
      "  638, number of output rows => 876\n"
     ]
    }
   ],
   "source": [
    "stagemetrics.print_accumulables() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
